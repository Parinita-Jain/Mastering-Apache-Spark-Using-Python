{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "\n",
    "\n",
    "<center> <h1> Shared Variables: Accumulators </h1> </center> \n",
    "\n",
    "---\n",
    "\n",
    " * **They are only \"added\" to through an associative and commutative operation.**\n",
    " * **They can be used to implement counters or sums.** \n",
    " * **Spark natively supports accumulators of numeric types, and programmers can add support for new types**\n",
    "\n",
    "\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Creating the Spark Context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the spark context\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"accumulator\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### `Initialize the accumulator & create a sample RDD`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the accumulator\n",
    "accuSum=spark.sparkContext.accumulator(0)\n",
    "\n",
    "# create a RDD of empty list\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Define the function`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the commutative function\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum+=x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Get the sum`\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sum of accumulator\n",
    "rdd.foreach(countFun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the value\n",
    "accuSum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Does an empty partition will be used to compute the results of accumulator variable?\n",
    "\n",
    "-  First of all, we will intialize the accumulator variable with 0.\n",
    "-  We will create a list of numbers and create rdd of it.\n",
    "-  Check the number of paritions.\n",
    "-  Check for the empty partitions.  \n",
    "-  Add one to each accumulator and get the result.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement a counter\n",
    "accumCount=spark.sparkContext.accumulator(0)\n",
    "\n",
    "# create a list of numbers and create its RDD\n",
    "rdd2=spark.sparkContext.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of partitions\n",
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, we have just 5 numbers in the list and the rdd is created with default number of paritions which is 8. So, 3 of the 8 partitions must be empty. Let's verify it with the help of the `glom` function.\n",
    "\n",
    "---\n",
    "\n",
    "#### `Check for the empty partitions`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [1], [], [2], [3], [], [4], [5]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the empty partitions\n",
    "rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, we will add 1 to each accumulator value and get the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add 1 to each value\n",
    "rdd2.foreach(lambda x:accumCount.add(1))\n",
    "\n",
    "# final result\n",
    "accumCount.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So, we got the value 5, therefore the empty partitions will not be used to compute the results of accumulator variable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
